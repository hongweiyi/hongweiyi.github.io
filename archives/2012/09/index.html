<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 小e的笔记</title><meta name="description" content="A Blog Powered By Hexo"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="http://fonts.useso.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="http://weibo.com/1674333040" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://twitter.com/hongwei89" target="_blank" class="nav-list-link">TWITTER</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2012/09/mapred-optimize-writable/" class="post-title-link">MapReduce优化（二） —— 善用Writable</a></h2><div class="post-meta"><div class="post-time">Sep 27, 2012</div></div><div class="post-content"><p><strong>一、简述</strong></p>
<p>上文主要是数据压缩的角度来分析了MapReduce压缩临时数据的优化，参见：<a href="http://www.hongweiyi.com/2012/02/mapred-optimize/">MapReduce优化（一）</a>。而这篇会更多的从代码层面说MR任务优化。</p>
<p>MapReduce大多数任务都是做日志分析，而一般的日志分析也就是高级点的WordCount程序：读入一段文本 -&gt; 获取需要的信息 -&gt; 统计输出。<br></div><a href="2012/09/mapred-optimize-writable/" class="read-more">... more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2012/09/hadoop-bug-in-text/" class="post-title-link">发现Hadoop小bug一枚</a></h2><div class="post-meta"><div class="post-time">Sep 18, 2012</div></div><div class="post-content"><p>最近在做一个任务，合并大小文本文件，每个不到5M，合并之后再进行统计分析。但是统计分析结果不对，本来只有60w个文件，但是最后的结果竟然到了200w了，断断续续debug了几天，从合并、map过程、combine过程、reduce过程，都一一过了一遍。最后发现在map过程中的数据就有重复，而且重复的形式很奇怪。<br></div><a href="2012/09/hadoop-bug-in-text/" class="read-more">... more</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2012/09/apache-hadoop-yarn-background-and-an-overview/" class="post-title-link">Apache Hadoop YARN - 背景及概述</a></h2><div class="post-meta"><div class="post-time">Sep 6, 2012</div></div><div class="post-content"><p>虽然yahoo!关于YARN作为下一代（Next-gen）MapReduce框架的文章（<a href="http://developer.yahoo.com/blogs/hadoop/posts/2011/02/mapreduce-nextgen/">点这里</a>）去年就看过了，但是那个看到是“下一代”，竟然以为只是一个设想，没想到早就发布了版本，导致对于Hadoop的认识还停留在0.20×版本上，真是罪过罪过。由于最近比较忙，闲暇时间扫了扫国内外博客，发现0.23、1.×，以及最近发布的2.×，hadoop的变化非常之大。比如说HDFS Federation（联邦）支持多NameNode并存，也有HA的BackupNode，想多了解的可以看<a href="http://ai-longyu.iteye.com/blog/1566619">这里</a>以及<a href="http://hadoop.apache.org/common/docs/r0.23.0/hadoop-yarn/hadoop-yarn-site/Federation.html">官方文档</a>。最大的莫过于计算框架了，MapReduce进入了2.0时代，MR2.0或者叫YARN（其实YARN和MapReduce没什么关系了），这篇博客就简要的说说Apache Hadoop MapReduce的前世今生吧。主要是翻译了这篇博客：<a href="http://hortonworks.com/blog/apache-hadoop-yarn-background-and-an-overview/">地址</a>，也加上了自己的一些见解，后续再继续添加对YARN的认识。<br></div><a href="2012/09/apache-hadoop-yarn-background-and-an-overview/" class="read-more">... more</a></article></li></ul></section><footer><div class="paginator"><a class="prev"> </a><a class="prev"> </a></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-66911097-1",'auto');ga('send','pageview');</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>