<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 发现Hadoop小bug一枚 · 小e的笔记</title><meta name="description" content="最近在做一个任务，合并大小文本文件，每个不到5M，合并之后再进行统计分析。但是统计分析结果不对，本来只有60w个文件，但是最后的结果竟然到了200w了，断断续续debug了几天，从合并、map过程、combine过程、reduce过程，都一一过了一遍。最后发现在map过程中的数据就有重复，而且重复的形式很奇怪。  
举例来说，SequenceFile中两个记录一个4M一个为1M，map第一个4M的记录没出现问题，但是map第二个1M的记录出问题了，记录大小为1M，但是getBytes()之后会获得4M的数据，前面1M是正常数据，而后面3M是非法的前一个记录的后3M数据。
开始以为是jvm没有将内存给销毁，但是觉着不对劲，就直接跟进代码瞅了瞅几眼。原来在map过程中，每次获得的key和value均会复用，而不会销毁，而Text的数据均存在了一个bytes数组，Text对象不销毁，bytes数组也不会销毁。可以参见：LineRecordReader or SequenceFileRecordReader的nextKeyValue()方法，该方法由Mapper的run方法间接调用。
继续刚才的那个问题，我在读取Text中的数据时，会调用text.getBytes()方法，逻辑上它应该是要返回leng"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="http://fonts.useso.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="http://weibo.com/1674333040" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://twitter.com/hongwei89" target="_blank" class="nav-list-link">TWITTER</a></li><li class="nav-list-item"><a href="/archives" target="_self" class="nav-list-link">ARCHIVE</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">发现Hadoop小bug一枚</h1><div class="post-meta"><div class="post-time">Sep 18, 2012 | [<a class="tag-link" href="/tags/Hadoop/">Hadoop</a>]</div></div><div class="post-content"><p>最近在做一个任务，合并大小文本文件，每个不到5M，合并之后再进行统计分析。但是统计分析结果不对，本来只有60w个文件，但是最后的结果竟然到了200w了，断断续续debug了几天，从合并、map过程、combine过程、reduce过程，都一一过了一遍。最后发现在map过程中的数据就有重复，而且重复的形式很奇怪。<br><a id="more"></a>  </p>
<p>举例来说，SequenceFile中两个记录一个4M一个为1M，map第一个4M的记录没出现问题，但是map第二个1M的记录出问题了，记录大小为1M，但是getBytes()之后会获得4M的数据，前面1M是正常数据，而后面3M是非法的前一个记录的后3M数据。</p>
<p>开始以为是jvm没有将内存给销毁，但是觉着不对劲，就直接跟进代码瞅了瞅几眼。原来在map过程中，每次获得的key和value均会复用，而不会销毁，而Text的数据均存在了一个bytes数组，Text对象不销毁，bytes数组也不会销毁。可以参见：LineRecordReader or SequenceFileRecordReader的nextKeyValue()方法，该方法由Mapper的run方法间接调用。</p>
<p>继续刚才的那个问题，我在读取Text中的数据时，会调用text.getBytes()方法，逻辑上它应该是要返回length长度的bytes给开发者，但是它却将整个bytes数组返回了。</p>
<p>心想着可以提交一个patch，但后来翻了一下新版本的源码，发现hadoop已经提供了一种解决方案——copyBytes()。具体实现见下图：</p>
<p><a href="/images/2012/09/Image.png"><img src="/images/2012/09/Image_thumb.png" alt="Image" title="Image"></a></p>
<p><strong>P.S.:</strong> Hadoop会在不变动原有逻辑的基础上进行修改，这样的话可以最大限度的减少对用户的影响，并且可以往下兼容。值得学习啊，给我的话，就直接修改getBytes()方法了。</p>
<p><strong>P.P.S.:</strong> 我是基于0.20.203.0做的实验，这个问题在0.22之后的版本均提供了解决方案。</p>
</div></article></div></section><footer><div class="paginator"><a href="/2012/09/mapred-optimize-writable/" class="prev">上一篇</a><a href="/2012/09/apache-hadoop-yarn-background-and-an-overview/" class="next">下一篇</a></div><div data-thread-key="2012/09/hadoop-bug-in-text/" data-title="发现Hadoop小bug一枚" data-url="http://hongweiyi.com/2012/09/hadoop-bug-in-text/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"yihongwei"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-66911097-1",'auto');ga('send','pageview');</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>