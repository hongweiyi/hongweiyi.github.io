<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Hadoop Pipes编程 · 小e的笔记</title><meta name="description" content="Hadoop Pipes编程 - Hongwei Yi"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="http://fonts.useso.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/tags" target="_self" class="nav-list-link">TAGS</a></li><li class="nav-list-item"><a href="http://weibo.com/1674333040" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://twitter.com/hongwei89" target="_blank" class="nav-list-link">TWITTER</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Hadoop Pipes编程</h1><div class="post-meta"><div class="post-time">May 12, 2012 | [<a class="tag-link" href="/tags/Hadoop/">Hadoop</a>, <a class="tag-link" href="/tags/Hadoop-Pipes/">Hadoop Pipes</a>, <a class="tag-link" href="/tags/MapReduce/">MapReduce</a>]</div></div><div class="post-content"><h3 id="1、Hadoop_Pipes简介">1、Hadoop Pipes简介</h3><p>Hadoop Pipes是Hadoop MapReduce的C++接口代称。不同于使用标准输入和输出来实现的map代码和reduce代码之间的Streaming编程，Pipes使用Socket作为TaskTracker与C++进程之间数据传输的通道，数据传输为字节流。</p>
<a id="more"></a>
<h3 id="2、Hadoop_Pipes编程初探">2、Hadoop Pipes编程初探</h3><p>Hadoop Pipes可供开发者编写RecordReader、Mapper、Partitioner、Reducer、RecordWriter五个组件，当然，也可以自定义Combiner。</p>
<p>网上有一大堆Hadoop Pipes的WordCount，个人觉得最好的WordCount还是Hadoop自带的，可以参见目录：$HADOOP_HOME/src/examples/pipes/impl</p>
<p>与Pipes相关的头文件放在了目录：</p>
<blockquote>
<p>$HADOOP_HOME/c++/Linux-i386oramd64-32/include/hadoop/  </p>
</blockquote>
<p>主要的文件为Pipes.hh，该头文件定义了一些抽象类，除去开发者需要编写的五大组件之外，还有JobConf、TaskContext、Closeable、Factory四个。</p>
<p>TaskContext：开发者可以从context中获取当前的key，value，progress和inputSplit等数据信息，当然，比较重要的就是调用emit将结果回传给Hadoop Framework。除了TaskContext，还有MapContext与ReduceContext，代码见下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> TaskContext &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  <span class="keyword">class</span> Counter &#123;  </span><br><span class="line">  <span class="keyword">private</span>:  </span><br><span class="line">    <span class="keyword">int</span> id;  </span><br><span class="line">  <span class="keyword">public</span>:  </span><br><span class="line">    Counter(<span class="keyword">int</span> counterId) : id(counterId) &#123;&#125;  </span><br><span class="line">    Counter(<span class="keyword">const</span> Counter&amp; counter) : id(counter.id) &#123;&#125;  </span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> id; &#125;  </span><br><span class="line">  &#125;;  </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">const</span> JobConf* <span class="title">getJobConf</span><span class="params">()</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="keyword">virtual</span> <span class="keyword">const</span> <span class="built_in">std</span>::<span class="function"><span class="built_in">string</span>&amp; <span class="title">getInputKey</span><span class="params">()</span> </span>= <span class="number">0</span>;   </span><br><span class="line">  <span class="keyword">virtual</span> <span class="keyword">const</span> <span class="built_in">std</span>::<span class="function"><span class="built_in">string</span>&amp; <span class="title">getInputValue</span><span class="params">()</span> </span>= <span class="number">0</span>;    </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">emit</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; key, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; value)</span> </span>= <span class="number">0</span>;    </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">progress</span><span class="params">()</span> </span>= <span class="number">0</span>;    </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">setStatus</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; status)</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> Counter*   </span><br><span class="line"><span class="title">getCounter</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; group, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name)</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">incrementCounter</span><span class="params">(<span class="keyword">const</span> Counter* counter, uint64_t amount)</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="keyword">virtual</span> ~TaskContext() &#123;&#125;  </span><br><span class="line">&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> MapContext: <span class="keyword">public</span> TaskContext &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  <span class="keyword">virtual</span> <span class="keyword">const</span> <span class="built_in">std</span>::<span class="function"><span class="built_in">string</span>&amp; <span class="title">getInputSplit</span><span class="params">()</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="keyword">virtual</span> <span class="keyword">const</span> <span class="built_in">std</span>::<span class="function"><span class="built_in">string</span>&amp; <span class="title">getInputKeyClass</span><span class="params">()</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="keyword">virtual</span> <span class="keyword">const</span> <span class="built_in">std</span>::<span class="function"><span class="built_in">string</span>&amp; <span class="title">getInputValueClass</span><span class="params">()</span> </span>= <span class="number">0</span>;  </span><br><span class="line">&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> ReduceContext: <span class="keyword">public</span> TaskContext &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">nextValue</span><span class="params">()</span> </span>= <span class="number">0</span>;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>JobConf：开发者可以通过获得任务的属性</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> JobConf &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">hasKey</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; key)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="keyword">virtual</span> <span class="keyword">const</span> <span class="built_in">std</span>::<span class="function"><span class="built_in">string</span>&amp; <span class="title">get</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; key)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">getInt</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; key)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">float</span> <span class="title">getFloat</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; key)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">getBoolean</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp;key)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;  </span><br><span class="line">  <span class="keyword">virtual</span> ~JobConf() &#123;&#125;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>Closeable：这个抽象类是五大组件的基类，只有两个方法，一个close()，一个析构函数。这个设计还是挺有Java风格的。</p>
<p>Factory：一个抽象工厂，用来创建五大组件的类，是模版工厂的基类。具体的可以参见TemplateFactory.hh。开发者在调用runTask时，创建相应的Factory传入即可。</p>
<h3 id="3、Hadoop_Pipes编程">3、Hadoop Pipes编程</h3><p>有了以上的基础知识，就可以开始编写MapReduce任务了。我们可以直接从examples着手，先来看看wordcount-simple.cc。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// wordcount-simple.cc -&gt; Mapper &amp; Reducer</span></span><br><span class="line"><span class="keyword">class</span> WordCountMap: <span class="keyword">public</span> HadoopPipes::Mapper &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  HadoopPipes::TaskContext::Counter* inputWords;  </span><br><span class="line"></span><br><span class="line">  WordCountMap(HadoopPipes::TaskContext&amp; context) &#123;  </span><br><span class="line">    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);  </span><br><span class="line">  &#125;  </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">map</span><span class="params">(HadoopPipes::MapContext&amp; context)</span> </span>&#123;  </span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="built_in">std</span>::<span class="built_in">string</span>&gt; words =   </span><br><span class="line">      HadoopUtils::splitString(context.getInputValue(), <span class="string">" "</span>);  </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i=<span class="number">0</span>; i &lt; words.size(); ++i) &#123;  </span><br><span class="line">      context.emit(words[i], <span class="string">"1"</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    context.incrementCounter(inputWords, words.size());  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> WordCountReduce: <span class="keyword">public</span> HadoopPipes::Reducer &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  HadoopPipes::TaskContext::Counter* outputWords;  </span><br><span class="line"></span><br><span class="line">  WordCountReduce(HadoopPipes::TaskContext&amp; context) &#123;  </span><br><span class="line">    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);  </span><br><span class="line">  &#125;  </span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">reduce</span><span class="params">(HadoopPipes::ReduceContext&amp; context)</span> </span>&#123;  </span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;  </span><br><span class="line">    <span class="keyword">while</span> (context.nextValue()) &#123;  </span><br><span class="line">      sum += HadoopUtils::toInt(context.getInputValue());  </span><br><span class="line">    &#125;  </span><br><span class="line">    context.emit(context.getInputKey(), HadoopUtils::toString(sum));  </span><br><span class="line">    context.incrementCounter(outputWords, <span class="number">1</span>);   </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;;</span><br><span class="line">```  </span><br><span class="line"></span><br><span class="line">该任务编写了两个主要组件，mapper与reducer。要实现这两个组件需要继承相应的基类。基类声明如下：</span><br><span class="line"></span><br><span class="line">``` c</span><br><span class="line"><span class="keyword">class</span> Mapper: <span class="keyword">public</span> Closable &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(MapContext&amp; context)</span> </span>= <span class="number">0</span>;  </span><br><span class="line">&#125;;  </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> Reducer: <span class="keyword">public</span> Closable &#123;  </span><br><span class="line"><span class="keyword">public</span>:  </span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(ReduceContext&amp; context)</span> </span>= <span class="number">0</span>;  </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>继承了相应的基类，就可以大胆的通过context获得key/value实现自己的逻辑了，结果处理完毕后，需要通过context.emit(key, value)将结果发送到下一阶段。</p>
<p>注：</p>
<ol>
<li>由于Factory创建对象需要传入Context对象，所以还需要实现一个构造函数，参数为TaskContext。</li>
<li>Hadoop Pipes内部规定，map与reduce的key/value均为Text类型，在C++中表现为string类型。不过，Hadoop还是做得比较贴心，有专门的方法负责处理string，具体可以参见StringUtils.hh。</li>
<li>Counter可以称之为统计器，可供开发者统计一些需要的数据，如读入行数、处理字节数等。任务完毕后，可以在web控制参看结果。</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// wordcount-part.cc -&gt; Partitioner</span></span><br><span class="line"><span class="keyword">class</span> WordCountPartitioner: <span class="keyword">public</span> HadoopPipes::Partitioner &#123;</span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">    WordCountPartitioner(HadoopPipes::TaskContext&amp; context)&#123;&#125;</span><br><span class="line">         <span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; key, <span class="keyword">int</span> numOfReduces)</span> </span>&#123;   </span><br><span class="line">           <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>该实例在提供简单Mapper与Reducer方法的同时，还提供了Partitioner，实例实现较为简单，直接返回了第一个reduce位置。开发者自定义的Partitioner同mapper/reducer一致，需要继承其基类HadoopPipes:: RecordWriter，也需要提供一个传入TaskContext的构造函数，它的声明如下：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> Partitioner &#123;   </span><br><span class="line">   <span class="keyword">public</span>:   </span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; key, <span class="keyword">int</span> numOfReduces)</span> </span>= <span class="number">0</span>;   </span><br><span class="line">    <span class="keyword">virtual</span> ~Partitioner() &#123;&#125;   </span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>Partitioner编写方法与Java的一致，对于partition方法，框架会自动为它传入两个参数，分别为key值和reduce task的个数numOfReduces，用户只需返回一个0~ numOfReduces-1的值即可。</p>
<p>wordcount-nopipe.cc -&gt; RecordReader &amp; RecordWriter</p>
<p>这个实例的命名让我思考了很久，是nopipe还是nopart呢？该实例没有实现Partitioner，实现了RecordReader与RecordWriter。框架在运行之初，检查到开发者没有使用Java内置的RecordWriter，所以就只将InputSplit信息通过Pipes发送给C++ Task，由Task实现自身的Record读方法。同样，在Record写数据时任务也没走Pipes，直接将数据写到了相应的位置，写临时文件会直接写到磁盘，写HDFS则需要通过libhdfs进行写操作。具体Pipes运行流程，请参见下篇博文。</p>
<p>RecordReader/RecordWriter实现较长，这里就不贴了，贴一下这俩的基类：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> RecordReader: <span class="keyword">public</span> Closable &#123;   </span><br><span class="line">   <span class="keyword">public</span>:   </span><br><span class="line">   <span class="function"><span class="keyword">virtual</span> <span class="keyword">bool</span> <span class="title">next</span><span class="params">(<span class="built_in">std</span>::<span class="built_in">string</span>&amp; key, <span class="built_in">std</span>::<span class="built_in">string</span>&amp; value)</span> </span>= <span class="number">0</span>;   <span class="number">4.</span>   <span class="comment">// 读进度   </span></span><br><span class="line">      <span class="function"><span class="keyword">virtual</span> <span class="keyword">float</span> <span class="title">getProgress</span><span class="params">()</span> </span>= <span class="number">0</span>;   </span><br><span class="line">&#125;;   </span><br><span class="line"><span class="keyword">class</span> RecordWriter: <span class="keyword">public</span> Closable &#123;   </span><br><span class="line">   <span class="keyword">public</span>:   </span><br><span class="line">   <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">emit</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; key,</span><br><span class="line">                     <span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; value)</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line">对于RecordReader，用户自定义的构造函数需携带类型为HadoopPipes::MapContext的参数（而不能是TaskContext），通过该参数的getInputSplit()的方法，用户可以获取经过序列化的InpuSplit对象，Java端采用不同的InputFormat可导致InputSplit对象格式不同，但对于大多数InpuSplit对象，它们可以提供至少三个信息：当前要处理的InputSplit所在的文件名，所在文件中的偏移量，它的长度。用户获取这三个信息后，可使用libhdfs库读取文件，以实现next方法。</span><br><span class="line"></span><br><span class="line">用户自定的RecordWriter的构造函数需携带参数TaskContext，通过该参数的getJobConf()可获取一个HadoopPipes::JobConf的对象，用户可从该对象中获取该reduce task的各种参数，如：该reduce task的编号（这对于确定输出文件名有用），reduce task的输出目录等。同时实现emit方法，将数据写入文件。</span><br><span class="line"></span><br><span class="line"><span class="preprocessor">### <span class="number">4</span>、Hadoop Pipes任务提交</span></span><br><span class="line"></span><br><span class="line">Hadoop Pipes任务提交命令根据Hadoop版本而不一，主体的命令有如下：</span><br></pre></td></tr></table></figure>
<p>hadoop pipes [-conf <path></path>] [-D <key=value>, <key=value>, …] [-input <path></path>] [-output <path></path>] [-jar <jar file="">] [-inputformat <class>] [-map <class>] [-partitioner <class>] [-reduce <class>] [-writer <class>] [-program <executable>]<br>```</executable></class></class></class></class></class></jar></key=value></key=value></p>
<p>想使用其它静态数据的话，还可以使用-files命令，该命令就是DistributedCache，直接将静态数据分发到所有datanode上。具体机制参见：<a href="http://www.hongweiyi.com/2012/02/iterative-mapred-distcache/" target="_blank" rel="external">DistributedCache</a>。使用如下：</p>
<blockquote>
<p>shell: bin/hadoop pipes … -files dict.txt</p>
<p>c: file = fopen(“dict.txt”, “r”); // 直接根据文件名读取  </p>
</blockquote>
<h3 id="5、小结">5、小结</h3><p>本篇博文简要了说了一下Hadoop Pipes的使用方法，下篇博文会对Hadoop Pipes的运行机制进行一个深入的讲解。</p>
<p>在这里贴一下董的优化意见：为了提高系能，RecordReader和RecordWriter最好采用Java代码实现（或者重用Hadoop中自带的），这是因为Hadoop自带的C++库libhdfs采用JNI实现，底层还是要调用Java相关接口，效率很低，此外，如果要处理的文件为二进制文件或者其他非文本文件，libhdfs可能不好处理。</p>
<blockquote>
<p>参考资料：</p>
<p>董的博客: <a href="http://dongxicheng.org/mapreduce/hadoop-pipes-programming/" target="_blank" rel="external">Hadoop pipes编程</a></p>
<p>《Hadoop权威指南》</p>
</blockquote>
</div></article></div></section><footer><div class="paginator"><a href="/2012/05/hadoop-pipes-src/" class="prev">上一篇</a><a href="/2012/04/nlp-say-hi/" class="next">下一篇</a></div><div data-thread-key="2012/05/hadoop-pipes/" data-title="Hadoop Pipes编程" data-url="http://hongweiyi.com/2012/05/hadoop-pipes/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"yihongwei"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script><div class="copyright"><p>© 2015 - 2016 <a href="http://hongweiyi.com">Hongwei Yi</a>, unless otherwise noted.</p></div></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-66911097-1",'auto');ga('send','pageview');</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>