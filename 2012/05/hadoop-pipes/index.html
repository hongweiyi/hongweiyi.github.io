<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Hadoop Pipes编程 · 小e的笔记</title><meta name="description" content="1**、Hadoop Pipes**简介
Hadoop Pipes是Hadoop MapReduce的C++接口代称。不同于使用标准输入和输出来实现的map代码和reduce代码之间的Streaming编程，Pipes使用Socket作为TaskTracker与C++进程之间数据传输的通道，数据传输为字节流。

2**、Hadoop Pipes**编程初探
Hadoop Pipes可供开发者编写RecordReader、Mapper、Partitioner、Reducer、RecordWriter五个组件，当然，也可以自定义Combiner。
网上有一大堆Hadoop Pipes的WordCount，个人觉得最好的WordCount还是Hadoop自带的，可以参见目录：$HADOOP_HOME/src/examples/pipes/impl
与Pipes相关的头文件放在了目录：

$HADOOP_HOME/c++/Linux-i386oramd64-32/include/hadoop/  
&lt;/"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="http://fonts.useso.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="http://weibo.com/1674333040" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://twitter.com/hongwei89" target="_blank" class="nav-list-link">TWITTER</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="/tags" target="_self" class="nav-list-link">TAGS</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Hadoop Pipes编程</h1><div class="post-meta"><div class="post-time">May 12, 2012</div></div><div class="post-content"><p><strong>1**</strong>、Hadoop Pipes<strong>**简介</strong></p>
<p>Hadoop Pipes是Hadoop MapReduce的C++接口代称。不同于使用标准输入和输出来实现的map代码和reduce代码之间的Streaming编程，Pipes使用Socket作为TaskTracker与C++进程之间数据传输的通道，数据传输为字节流。</p>
<a id="more"></a>
<p><strong>2**</strong>、Hadoop Pipes<strong>**编程初探</strong></p>
<p>Hadoop Pipes可供开发者编写RecordReader、Mapper、Partitioner、Reducer、RecordWriter五个组件，当然，也可以自定义Combiner。</p>
<p>网上有一大堆Hadoop Pipes的WordCount，个人觉得最好的WordCount还是Hadoop自带的，可以参见目录：$HADOOP_HOME/src/examples/pipes/impl</p>
<p>与Pipes相关的头文件放在了目录：</p>
<blockquote>
<p>$HADOOP_HOME/c++/Linux-i386oramd64-32/include/hadoop/  </p>
</blockquote>
<p>主要的文件为Pipes.hh，该头文件定义了一些抽象类，除去开发者需要编写的五大组件之外，还有JobConf、TaskContext、Closeable、Factory四个。</p>
<p><strong>TaskContext**</strong>：**开发者可以从context中获取当前的key，value，progress和inputSplit等数据信息，当然，比较重要的就是调用emit将结果回传给Hadoop Framework。除了TaskContext，还有MapContext与ReduceContext，代码见下：<br>  <div class="dp-highlighter">   <div class="bar"></div>    </div></p>
<ol>
<li><span><span class="keyword">class</span><span> TaskContext {&#160;&#160; </span></span></li>
<li><span></span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">class</span><span> Counter {&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">private</span><span>:&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; </span><span class="datatypes">int</span><span> id;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; Counter(</span><span class="datatypes">int</span><span> counterId) : id(counterId) {}&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; Counter(</span><span class="keyword">const</span><span> Counter&amp; counter) : id(counter.id) {}&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; </span><span class="datatypes">int</span><span> getId() </span><span class="keyword">const</span><span> { </span><span class="keyword">return</span><span> id; }&#160;&#160; </span></li>
<li><span>&#160; };&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">const</span><span> JobConf* getJobConf() = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">const</span><span> std::string&amp; getInputKey() = 0;&#160;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">const</span><span> std::string&amp; getInputValue() = 0;&#160;&#160;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">void</span><span> emit(</span><span class="keyword">const</span><span> std::string&amp; key, </span><span class="keyword">const</span><span> std::string&amp; value) = 0;&#160;&#160;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">void</span><span> progress() = 0;&#160;&#160;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">void</span><span> setStatus(</span><span class="keyword">const</span><span> std::string&amp; status) = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span> Counter*&#160;&#160;&#160; </span></li>
<li><span>getCounter(</span><span class="keyword">const</span><span> std::string&amp; group, </span><span class="keyword">const</span><span> std::string&amp; name) = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">void</span><span> incrementCounter(</span><span class="keyword">const</span><span> Counter* counter, uint64_t amount) = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span> ~TaskContext() {}&#160;&#160; </span></li>
<li><span>};&#160;&#160; </span></li>
<li><span>&#160; </span></li>
<li><span></span><span class="keyword">class</span><span> MapContext: </span><span class="keyword">public</span><span> TaskContext {&#160;&#160; </span></li>
<li><span></span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">const</span><span> std::string&amp; getInputSplit() = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">const</span><span> std::string&amp; getInputKeyClass() = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">const</span><span> std::string&amp; getInputValueClass() = 0;&#160;&#160; </span></li>
<li><span>};&#160;&#160; </span></li>
<li><span>&#160; </span></li>
<li><span></span><span class="keyword">class</span><span> ReduceContext: </span><span class="keyword">public</span><span> TaskContext {&#160;&#160; </span></li>
<li><span></span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">bool</span><span> nextValue() = 0;&#160;&#160; </span></li>
<li><span>};&#160;&#160; </span>     </li>
</ol>
<p><strong>JobConf</strong>：开发者可以通过获得任务的属性<br>  <div class="dp-highlighter">   <div class="bar"></div>    </div></p>
<ol>
<li><span><span class="keyword">class</span><span> JobConf {&#160;&#160; </span></span></li>
<li><span></span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">bool</span><span> hasKey(</span><span class="keyword">const</span><span> std::string&amp; key) </span><span class="keyword">const</span><span> = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">const</span><span> std::string&amp; get(</span><span class="keyword">const</span><span> std::string&amp; key) </span><span class="keyword">const</span><span> = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">int</span><span> getInt(</span><span class="keyword">const</span><span> std::string&amp; key) </span><span class="keyword">const</span><span> = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">float</span><span> getFloat(</span><span class="keyword">const</span><span> std::string&amp; key) </span><span class="keyword">const</span><span> = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">bool</span><span> getBoolean(</span><span class="keyword">const</span><span> std::string&amp;key) </span><span class="keyword">const</span><span> = 0;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span> ~JobConf() {}&#160;&#160; </span></li>
<li><span>};&#160;&#160; </span>     </li>
</ol>
<p><strong>Closeable</strong>：这个抽象类是五大组件的基类，只有两个方法，一个close()，一个析构函数。这个设计还是挺有Java风格的。</p>
<p><strong>Factory</strong>：一个抽象工厂，用来创建五大组件的类，是模版工厂的基类。具体的可以参见TemplateFactory.hh。开发者在调用runTask时，创建相应的Factory传入即可。</p>
<p><strong>3**</strong>、Hadoop Pipes<strong>**编程</strong></p>
<p>有了以上的基础知识，就可以开始编写MapReduce任务了。我们可以直接从examples着手，先来看看wordcount-simple.cc。</p>
<p><strong>wordcount-simple.cc -&gt; Mapper &amp; Reducer</strong><br>  <div class="dp-highlighter">   <div class="bar"></div>    </div></p>
<ol>
<li><span><span class="keyword">class</span><span> WordCountMap: </span><span class="keyword">public</span><span> HadoopPipes::Mapper {&#160;&#160; </span></span></li>
<li><span></span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160; HadoopPipes::TaskContext::Counter* inputWords;&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160;&#160; </span></li>
<li><span>&#160; WordCountMap(HadoopPipes::TaskContext&amp; context) {&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);&#160;&#160; </span></li>
<li><span>&#160; }&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">void</span><span> map(HadoopPipes::MapContext&amp; context) {&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; std::vector&lt;std::string&gt; words =&#160;&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160;&#160;&#160; HadoopUtils::splitString(context.getInputValue(), </span><span class="string">&quot; &quot;</span><span>);&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; </span><span class="keyword">for</span><span>(unsigned </span><span class="datatypes">int</span><span> i=0; i &lt; words.size(); ++i) {&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160;&#160;&#160; context.emit(words[i], </span><span class="string">&quot;1&quot;</span><span>);&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; }&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; context.incrementCounter(inputWords, words.size());&#160;&#160; </span></li>
<li><span>&#160; }&#160;&#160; </span></li>
<li><span>};&#160;&#160; </span></li>
<li><span>&#160; </span></li>
<li><span></span><span class="keyword">class</span><span> WordCountReduce: </span><span class="keyword">public</span><span> HadoopPipes::Reducer {&#160;&#160; </span></li>
<li><span></span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160; HadoopPipes::TaskContext::Counter* outputWords;&#160;&#160; </span></li>
<li><span>&#160; </span></li>
<li><span>&#160; WordCountReduce(HadoopPipes::TaskContext&amp; context) {&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);&#160;&#160; </span></li>
<li><span>&#160; }&#160;&#160; </span></li>
<li><span>&#160; </span></li>
<li><span>&#160; </span><span class="keyword">void</span><span> reduce(HadoopPipes::ReduceContext&amp; context) {&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; </span><span class="datatypes">int</span><span> sum = 0;&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; </span><span class="keyword">while</span><span> (context.nextValue()) {&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160;&#160;&#160; sum += HadoopUtils::toInt(context.getInputValue());&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; }&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; context.emit(context.getInputKey(), HadoopUtils::toString(sum));&#160;&#160; </span></li>
<li><span>&#160;&#160;&#160; context.incrementCounter(outputWords, 1);&#160;&#160;&#160; </span></li>
<li><span>&#160; }&#160;&#160; </span></li>
<li><span>};&#160; </span>     </li>
</ol>
<p>该任务编写了两个主要组件，mapper与reducer。要实现这两个组件需要继承相应的基类。基类声明如下：<br>  <div class="dp-highlighter">   <div class="bar"></div>    </div></p>
<ol>
<li><span><span class="keyword">class</span><span> Mapper: </span><span class="keyword">public</span><span> Closable {&#160;&#160; </span></span></li>
<li><span></span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">void</span><span> map(MapContext&amp; context) = 0;&#160;&#160; </span></li>
<li><span>};&#160;&#160; </span></li>
<li><span>&#160; </span></li>
<li><span></span><span class="keyword">class</span><span> Reducer: </span><span class="keyword">public</span><span> Closable {&#160;&#160; </span></li>
<li><span></span><span class="keyword">public</span><span>:&#160;&#160; </span></li>
<li><span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">void</span><span> reduce(ReduceContext&amp; context) = 0;&#160;&#160; </span></li>
<li><span>};&#160;&#160; </span>     </li>
</ol>
<p>继承了相应的基类，就可以大胆的通过context获得key/value实现自己的逻辑了，结果处理完毕后，需要通过context.emit(key, value)将结果发送到下一阶段。</p>
<p>注：</p>
<p>1）由于Factory创建对象需要传入Context对象，所以还需要实现一个构造函数，参数为TaskContext。</p>
<p>2）Hadoop Pipes内部规定，map与reduce的key/value均为Text类型，在C++中表现为string类型。不过，Hadoop还是做得比较贴心，有专门的方法负责处理string，具体可以参见StringUtils.hh。</p>
<p>3）Counter可以称之为统计器，可供开发者统计一些需要的数据，如读入行数、处理字节数等。任务完毕后，可以在web控制参看结果。</p>
<p><strong>wordcount-part.cc -&gt; Partitioner</strong><br>  <div class="dp-highlighter">   <div class="bar"></div>    </div></p>
<ol>
<li><span><span class="keyword">class</span><span> WordCountPartitioner: </span><span class="keyword">public</span><span> HadoopPipes::Partitioner {&#160;&#160; </span></span>2.  <span></span><span class="keyword">public</span><span>:&#160;&#160; </span>3.  <span>&#160; WordCountPartitioner(HadoopPipes::TaskContext&amp; context){}&#160;&#160; </span>4.  <span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">int</span><span> partition(</span><span class="keyword">const</span><span> std::string&amp; key, </span><span class="datatypes">int</span><span> numOfReduces) {&#160;&#160; </span>5.  <span>&#160;&#160;&#160; </span><span class="keyword">return</span><span> 0;&#160;&#160; </span>6.  <span>&#160; }&#160;&#160; </span>7.  <span>};&#160;&#160; </span>     </li>
</ol>
<p>该实例在提供简单Mapper与Reducer方法的同时，还提供了Partitioner，实例实现较为简单，直接返回了第一个reduce位置。开发者自定义的Partitioner同mapper/reducer一致，需要继承其基类HadoopPipes:: RecordWriter，也需要提供一个传入TaskContext的构造函数，它的声明如下：<br>  <div class="dp-highlighter">   <div class="bar"></div>    </div></p>
<ol>
<li><span><span class="keyword">class</span><span> Partitioner {&#160;&#160; </span></span>2.  <span></span><span class="keyword">public</span><span>:&#160;&#160; </span>3.  <span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">int</span><span> partition(</span><span class="keyword">const</span><span> std::string&amp; key, </span><span class="datatypes">int</span><span> numOfReduces) = 0;&#160;&#160; </span>4.  <span>&#160; </span><span class="keyword">virtual</span><span> ~Partitioner() {}&#160;&#160; </span>5.  <span>};&#160; </span>     </li>
</ol>
<p>Partitioner编写方法与Java的一致，对于partition方法，框架会自动为它传入两个参数，分别为key值和reduce task的个数numOfReduces，用户只需返回一个0~ numOfReduces-1的值即可。</p>
<p><strong>wordcount-nopipe.cc -&gt; RecordReader &amp; RecordWriter</strong></p>
<p>这个实例的命名让我思考了很久，是nopipe还是nopart呢？该实例没有实现Partitioner，实现了RecordReader与RecordWriter。框架在运行之初，检查到开发者没有使用Java内置的RecordWriter，所以就只将InputSplit信息通过Pipes发送给C++ Task，由Task实现自身的Record读方法。同样，在Record写数据时任务也没走Pipes，直接将数据写到了相应的位置，写临时文件会直接写到磁盘，写HDFS则需要通过libhdfs进行写操作。具体Pipes运行流程，请参见下篇博文。</p>
<p>RecordReader/RecordWriter实现较长，这里就不贴了，贴一下这俩的基类：<br>  <div class="dp-highlighter">   <div class="bar"></div>    </div></p>
<ol>
<li><span><span class="keyword">class</span><span> RecordReader: </span><span class="keyword">public</span><span> Closable {&#160;&#160; </span></span>2.  <span></span><span class="keyword">public</span><span>:&#160;&#160; </span>3.  <span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">bool</span><span> next(std::string&amp; key, std::string&amp; value) = 0;&#160;&#160; </span>4.  <span>&#160; </span><span class="comment">// 读进度 </span><span>&#160; </span>5.  <span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="datatypes">float</span><span> getProgress() = 0;&#160;&#160; </span>6.  <span>};&#160;&#160; </span>7.  <span>&#160; </span>8.  <span></span><span class="keyword">class</span><span> RecordWriter: </span><span class="keyword">public</span><span> Closable {&#160;&#160; </span>9.  <span></span><span class="keyword">public</span><span>:&#160;&#160; </span>10.  <span>&#160; </span><span class="keyword">virtual</span><span>&#160;</span><span class="keyword">void</span><span> emit(</span><span class="keyword">const</span><span> std::string&amp; key,&#160;&#160; </span>11.  <span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160; </span><span class="keyword">const</span><span> std::string&amp; value) = 0;&#160;&#160; </span>12.  <span>};&#160; </span>     </li>
</ol>
<p>对于RecordReader，用户自定义的构造函数需携带类型为HadoopPipes::MapContext的参数（而不能是TaskContext），通过该参数的getInputSplit()的方法，用户可以获取经过序列化的InpuSplit对象，Java端采用不同的InputFormat可导致InputSplit对象格式不同，但对于大多数InpuSplit对象，它们可以提供至少三个信息：当前要处理的InputSplit所在的文件名，所在文件中的偏移量，它的长度。用户获取这三个信息后，可使用libhdfs库读取文件，以实现next方法。</p>
<p>用户自定的RecordWriter的构造函数需携带参数TaskContext，通过该参数的getJobConf()可获取一个HadoopPipes::JobConf的对象，用户可从该对象中获取该reduce task的各种参数，如：该reduce task的编号（这对于确定输出文件名有用），reduce task的输出目录等。同时实现emit方法，将数据写入文件。</p>
<p><strong>4**</strong>、Hadoop Pipes<strong>**任务提交</strong></p>
<p>Hadoop Pipes任务提交命令根据Hadoop版本而不一，主体的命令有如下：</p>
<p>hadoop pipes [-conf &lt;path&gt;] [-D &lt;key=value&gt;, &lt;key=value&gt;, …] [-input &lt;path&gt;] [-output &lt;path&gt;] [-jar &lt;jar file&gt;] [-inputformat &lt;class&gt;] [-map &lt;class&gt;] [-partitioner &lt;class&gt;] [-reduce &lt;class&gt;] [-writer &lt;class&gt;] [-program &lt;executable&gt;]    <table border="0" cellspacing="1" cellpadding="0"><tbody>       <tr>         <td valign="top" width="254">           <p><strong>命**</strong> <strong>**令</strong><br>         </p></td>          <td valign="top" width="292">           </td></tr></tbody></table></p>
<p><strong>描**</strong> <strong>**述</strong><br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-conf &lt;path&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>任务配置文件<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-D &lt;key=value&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>添加单独的配置<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-input &lt;path&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>输入数据目录<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-output &lt;path&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>输出数据目录<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-jar &lt;jar file&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>应用程序jar包<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-inputformat class<br>                   <td valign="top" width="292">           </td></p>
<p>#Java版的InputFormat<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-map &lt;class&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>Java版的Mapper<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-partitioner &lt;class&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>Java版的Partitioner<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-reduce &lt;class&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>Java版的Reducer<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-writer &lt;class&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>Java版的 RecordWriter<br>                        <tr>         <td valign="top" width="254">           </td></tr></p>
<p>-program &lt;executable&gt;<br>                   <td valign="top" width="292">           </td></p>
<p>C++可执行程序<br>                      </p>  <p></p>
<p>想使用其它静态数据的话，还可以使用-files命令，该命令就是DistributedCache，直接将静态数据分发到所有datanode上。具体机制参见：<a href="http://www.hongweiyi.com/2012/02/iterative-mapred-distcache/" target="_blank" rel="external">DistributedCache</a>。使用如下：</p>
<blockquote>
<p><strong>shell</strong>: bin/hadoop pipes … -files dict.txt</p>
<p><strong>c</strong>: file = fopen(“dict.txt”, “r”); // 直接根据文件名读取  </p>
</blockquote>
<p><strong>5**</strong>、小结**</p>
<p>本篇博文简要了说了一下Hadoop Pipes的使用方法，下篇博文会对Hadoop Pipes的运行机制进行一个深入的讲解。</p>
<p>在这里贴一下董的优化意见：为了提高系能，RecordReader和RecordWriter最好采用Java代码实现（或者重用Hadoop中自带的），这是因为Hadoop自带的C++库libhdfs采用JNI实现，底层还是要调用Java相关接口，效率很低，此外，如果要处理的文件为二进制文件或者其他非文本文件，libhdfs可能不好处理。</p>
<p>&#160;</p>
<blockquote>
<p><strong>参考资料：</strong></p>
<p>董的博客: <a href="http://dongxicheng.org/mapreduce/hadoop-pipes-programming/" target="_blank" rel="external">Hadoop pipes编程</a></p>
<p>《Hadoop权威指南》</p>
</blockquote>
</div></article></div></section><footer><div class="paginator"><a href="/2012/05/hadoop-pipes-src/" class="prev">上一篇</a><a href="/2012/04/relax-schrodinger-cat/" class="next">下一篇</a></div><div data-thread-key="2012/05/hadoop-pipes/" data-title="Hadoop Pipes编程" data-url="http://hongweiyi.com/2012/05/hadoop-pipes/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"yihongwei"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script></footer><script>(function(b,o,i,l,e,r){b.GoogleAnalyticsObject=l;b[l]||(b[l]=function(){(b[l].q=b[l].q||[]).push(arguments)});b[l].l=+new Date;e=o.createElement(i);r=o.getElementsByTagName(i)[0];e.src='//www.google-analytics.com/analytics.js';r.parentNode.insertBefore(e,r)}(window,document,'script','ga'));ga('create',"UA-66911097-1",'auto');ga('send','pageview');</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>